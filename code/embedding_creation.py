"""
Unsupervised Building-to-Tract Embedding Pipeline using self-supervised contrastive learning

This script trains a dual-scale transformer model for generating unsupervised tract-level embeddings from per-building features. 
It implements self-supervised contrastive learning with data augmentation on building sequences within each tract.

**Workflow:**
- Loads per-building features, rotation, height and building type (preprocessed .npy/.npz/.csv files).
- Aggregates features by tract (GEOID).
- Applies scaling and data augmentation.
- Trains the DualScaleEmbedder model with contrastive loss and early stopping.
- Saves tract embeddings and trained model.

**Required input files** building_features.npy, building_rot_type.npz are generated by data preprocessing pipeline (from resnet.py)
and combined_building_list.csv is generated by raterization.py
"""

import os
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# ===============================
# CONFIGURE THESE VARIABLES 
# ===============================
CITY = "Chicago" #Chicago, Brookly, Los Angeles for our case
YEAR = "2013"  # We used 2013, 2018 and 2020 for two configurations
DATA_ROOT = "/data/folder"
OUTPUT_EMBEDDINGS = f"{CITY}_{YEAR}_building_embeddings.csv"
OUTPUT_MODEL = f"{CITY}_{YEAR}_dualscale_embedder.pth"


features_path = os.path.join(DATA_ROOT, CITY, f"Building Footprint {YEAR}_{CITY}", "Building list", "processed", "building_features.npy")
rot_type_file = os.path.join(DATA_ROOT, CITY, f"Building Footprint {YEAR}_{CITY}", "Building list", "building_rot_type.npz")
mapping_df_path = os.path.join(DATA_ROOT, CITY, f"Building Footprint {YEAR}_{CITY}", "Building list", "combined_building_list.csv")

# Data loading and arrangement 
building_features = np.load(features_path)
rot_type_array = np.load(rot_type_file, allow_pickle=True)["rot_type_mem"]
rotation_radians = np.arctan2(rot_type_array[:, 1], rot_type_array[:, 0]).reshape(-1, 1)
height = attributes[:, 2]
onehot_array = rot_type_array[:, 3:]
combined_features = np.concatenate([building_features, height, rotation_radians, onehot_array], axis=1)

mapping_df = pd.read_csv(mapping_df_path)
tract_dict = {}
for i, row in mapping_df.iterrows():
    tract = row['GEOID']
    if tract not in tract_dict:
        tract_dict[tract] = []
    tract_dict[tract].append(combined_features[i])
for tract in tract_dict:
    tract_dict[tract] = np.stack(tract_dict[tract], axis=0)
all_data = np.vstack(list(tract_dict.values()))
scaler = StandardScaler().fit(all_data)
for k in tract_dict:
    tract_dict[k] = scaler.transform(tract_dict[k])

# Model 
class DualScaleEmbedder(nn.Module):
    def __init__(self, d_building=75, d_hidden=64, d_feedforward=512, building_head=4, building_layers=3):
        super().__init__()
        self.building_projector = nn.Linear(d_building, d_hidden)
        self.local_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_hidden, nhead=building_head, dim_feedforward=d_feedforward // 2, batch_first=False),
            num_layers=1
        )
        self.global_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_hidden, nhead=building_head * 2, dim_feedforward=d_feedforward * 2, batch_first=False),
            num_layers=building_layers
        )
        self.proj_head = nn.Sequential(
            nn.Linear(d_hidden * 2, 128),
            nn.ReLU(),
            nn.Linear(128, 64)
        )
        self.contrastive_proj = nn.Sequential(
            nn.Linear(d_hidden * 2, 128),
            nn.ReLU(),
            nn.Linear(128, 64)
        )
    def forward(self, building_feature, building_mask):
        x = self.building_projector(building_feature)
        x = x.transpose(0, 1)
        padding_mask = building_mask.bool()
        local_feats = self.local_encoder(x, src_key_padding_mask=padding_mask)
        global_feats = self.global_encoder(x, src_key_padding_mask=padding_mask)
        def pool(feats, mask):
            feats = feats.transpose(0, 1)
            feats = feats * (~mask).unsqueeze(-1)
            return feats.sum(1) / (~mask).sum(1, keepdim=True).clamp(min=1e-5)
        local_pooled = pool(local_feats, padding_mask)
        global_pooled = pool(global_feats, padding_mask)
        combined = torch.cat([local_pooled, global_pooled], dim=-1)
        return self.proj_head(combined), self.contrastive_proj(combined)

def nt_xent_loss(embeddings, temperature=0.1):
    device = embeddings.device
    batch_size = embeddings.shape[0] // 2
    labels = torch.cat([torch.arange(batch_size) for _ in range(2)], dim=0).to(device)
    masks = torch.eye(2 * batch_size, dtype=torch.bool).to(device)
    sim = F.cosine_similarity(embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=2)
    sim = sim / temperature
    sim.masked_fill_(masks, -float('inf'))
    positives = torch.cat([
        torch.diag(sim, batch_size),
        torch.diag(sim, -batch_size)
    ])
    numerator = torch.exp(positives)
    denominator = torch.sum(torch.exp(sim), dim=1)
    loss = -torch.log(numerator / denominator)
    return loss.mean()

def augment_buildings(building_features, mask, p_drop=0.1):
    non_masked = torch.where(~mask)[0]
    n_non_masked = len(non_masked)
    n_drop = int(p_drop * n_non_masked)
    drop_idx = torch.randperm(n_non_masked)[:n_drop]
    aug_features = building_features.clone()
    if len(drop_idx) > 0:
        aug_features[non_masked[drop_idx]] = 0.0
    return aug_features

class UnsupervisedTractDataset(Dataset):
    def __init__(self, tract_dict, max_seq_len=None):
        self.tract_ids = list(tract_dict.keys())
        self.original_features = [torch.tensor(tract_dict[tid], dtype=torch.float32)
                                  for tid in self.tract_ids]
        if max_seq_len is None:
            self.max_seq_len = max(len(f) for f in self.original_features)
        else:
            self.max_seq_len = max_seq_len
        self.features = []
        self.masks = []
        for feats in self.original_features:
            padded, mask = self.pad_sequence(feats)
            self.features.append(padded)
            self.masks.append(mask)
    def pad_sequence(self, tensor):
        seq_len, d_features = tensor.shape
        mask = torch.ones(self.max_seq_len, dtype=torch.bool)
        padded = torch.zeros(self.max_seq_len, d_features)
        if seq_len > 0:
            padded[:seq_len] = tensor
            mask[:seq_len] = False
        return padded, mask
    def __len__(self):
        return len(self.tract_ids)
    def __getitem__(self, idx):
        return self.features[idx], self.masks[idx], self.tract_ids[idx]

def generate_embeddings(model, data_loader):
    model.eval()
    all_embeddings = []
    all_tract_ids = []
    with torch.no_grad():
        for buildings, masks, tract_ids in data_loader:
            buildings, masks = buildings.to(device), masks.to(device)
            embeddings, _ = model(buildings, masks)
            all_embeddings.append(embeddings.cpu())
            all_tract_ids.extend(tract_ids)
    return torch.cat(all_embeddings, dim=0), all_tract_ids

# Split and dataloaders
tract_ids = list(tract_dict.keys())
train_ids, val_ids = train_test_split(tract_ids, test_size=0.2, random_state=42)
train_dict = {tid: tract_dict[tid] for tid in train_ids}
val_dict = {tid: tract_dict[tid] for tid in val_ids}
train_dataset = UnsupervisedTractDataset(train_dict)
val_dataset = UnsupervisedTractDataset(val_dict)
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

def train_embedder(train_loader, val_loader, patience=3, max_epochs=50):
    model = DualScaleEmbedder().to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)
    best_val_loss = float('inf')
    epochs_no_improve = 0
    best_model_weights = None
    train_losses = []
    val_losses = []
    for epoch in range(max_epochs):
        model.train()
        train_loss = 0.0
        for buildings, masks, _ in train_loader:
            buildings, masks = buildings.to(device), masks.to(device)
            buildings_aug1 = buildings
            buildings_aug2 = torch.stack([
                augment_buildings(b, m) for b, m in zip(buildings, masks)
            ])
            _, proj1 = model(buildings_aug1, masks)
            _, proj2 = model(buildings_aug2, masks)
            proj_all = torch.cat([proj1, proj2], dim=0)
            loss = nt_xent_loss(proj_all)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for buildings, masks, _ in val_loader:
                buildings, masks = buildings.to(device), masks.to(device)
                buildings_aug1 = buildings
                buildings_aug2 = torch.stack([
                    augment_buildings(b, m) for b, m in zip(buildings, masks)
                ])
                _, proj1 = model(buildings_aug1, masks)
                _, proj2 = model(buildings_aug2, masks)
                proj_all = torch.cat([proj1, proj2], dim=0)
                val_loss += nt_xent_loss(proj_all).item()
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            epochs_no_improve = 0
            best_model_weights = model.state_dict()
            torch.save(model.state_dict(), OUTPUT_MODEL)
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                print(f'Early stopping triggered after {epoch+1} epochs!')
                break
    model.load_state_dict(best_model_weights)
    return model, train_losses, val_losses

trained_model, train_losses, val_losses = train_embedder(
    train_loader, val_loader, patience=3, max_epochs=50
)

full_dataset = UnsupervisedTractDataset(tract_dict)
full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)
embeddings, tract_ids = generate_embeddings(trained_model, full_loader)
embeddings_np = embeddings.numpy()
embedding_df = pd.DataFrame({
    'GEOID': tract_ids,
    **{f'embed_{i}': embeddings_np[:, i] for i in range(embeddings_np.shape[1])}
})
embedding_df.to_csv(OUTPUT_EMBEDDINGS, index=False)
print(f"Saved embeddings to {OUTPUT_EMBEDDINGS}")
torch.save(trained_model.state_dict(), OUTPUT_MODEL)
print(f"Saved model to {OUTPUT_MODEL}")

