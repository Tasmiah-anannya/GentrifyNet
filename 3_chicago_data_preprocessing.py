# -*- coding: utf-8 -*-
"""3. Chicago_data preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eCFrvmgRLlUv1a7ThuWcgNy46iaqv2lX
"""

from google.colab import drive
drive.mount('/content/drive')

pip install rasterio

"""#Creating image per building with rotation and height

Creating building_list
"""

import os
import glob
import pandas as pd
import numpy as np

csv_directory = "/content/drive/MyDrive/Research/USA data/Region embedding/Chicago/Building Footprint 2018 Chicago_new"
csv_files = glob.glob(os.path.join(csv_directory, "buildings_tract_*.csv"))
output_file="/content/drive/MyDrive/Research/USA data/Region embedding/Chicago/Building Footprint 2018 Chicago_new/Building list/combined_building_list_type.csv"

df_list = []

for csv_file in csv_files:
    print(csv_file)
    base_name = os.path.basename(csv_file)
    parts = base_name.split('_')
    if len(parts) >= 3:
        geoid = parts[2]
    else:
        geoid = np.nan  # If format is unexpected.

    df = pd.read_csv(csv_file)

     # Convert the 'height' column to numeric values, coercing errors to NaN.
    if 'height' in df.columns:
        df['height'] = pd.to_numeric(df['height'], errors='coerce')

    # Define the required columns.
    required_cols = ['geometry', 'height', 'nycdoitt:bin']

    # For each required column, if it's missing, create it with NaN.
    for col in required_cols:
        if col not in df.columns:
            df[col] = np.nan

    # Insert GEOID as the first column.
    df.insert(0, "GEOID", geoid)

    # Select only the desired columns (order: GEOID, geometry, height, nycdott:bin).
    df = df[['GEOID', 'geometry', 'height', 'nycdoitt:bin']]

    # Append the DataFrame to our list.
    df_list.append(df)

# Concatenate all DataFrames into one single DataFrame.
combined_df = pd.concat(df_list, ignore_index=True)

avg_heights = combined_df.groupby("GEOID")["height"].mean()

# Define a function to fill in missing height.
def fill_height(row):
    if pd.isnull(row["height"]):
        # If the GEOID exists in the average series, use that average.
        return avg_heights.get(row["GEOID"], np.nan)
    else:
        return row["height"]

# Apply the function to the DataFrame.
combined_df["height"] = combined_df.apply(fill_height, axis=1)
# --- End of missing value filling ---

print(combined_df.head())

combined_df.to_csv(output_file, index=False)

import pandas as pd

csv_file = "/content/drive/MyDrive/Research/USA data/Region embedding/Chicago/Building Footprint 2018 Chicago_new/Building list/combined_building_list_type.csv"

df = pd.read_csv(csv_file)

print(f"The CSV file has {len(df)} rows.")

print(len(csv_files))

"""Rasterization"""

import argparse
import math
import os
import pickle as pkl
import rasterio.features
import geopandas as gpd
import numpy as np
from scipy.spatial import KDTree
from shapely import affinity
from shapely.geometry import Point
from shapely.ops import unary_union
from tqdm import tqdm, trange
from shapely import wkt, affinity
import pandas as pd

import os
import numpy as np
import pandas as pd
import shapely.wkt as wkt
from shapely import affinity
import rasterio
from tqdm import trange
import rasterio.features

accommodation_tags = {
    "yes","apartments","barracks","bungalow","cabin","detached","annexe","dormitory","farm","ger",
    "hotel","house","houseboat","residential","semidetached_house","static_caravan","stilt_house",
    "terrace","tree_house","trullo"
}
commercial_tags = {"commercial","industrial","kiosk","office","retail","supermarket","warehouse"}
religious_tags = {"religious","cathedral","chapel","church","kingdom_hall","monastery","mosque",
                  "presbytery","shrine","synagogue","temple"}
amenity_tags = {"bakehouse","bridge","civic","college","fire_station","government","gatehouse","hospital",
                "kindergarten","museum","public","school","toilets","train_station","transportation","university"}
agriculture_tags = {"barn","conservatory","cowshed","farm_auxiliary","greenhouse","slurry_tank","stable","sty","livestock"}
sports_tags = {"grandstand","pavilion","riding_hall","sports_hall","sports_centre","stadium"}
storage_tags = {"allotment_house","boathouse","hanger","hut","shed"}
cars_tags = {"carport","garage","garages","parking"}
power_building_tags = {"digester","service","tech_cab","transformer_tower","water_tower","silo","storage_tank"}
other_building_tags = {
    "beach_hut","bunker","castle","construction","container","guardhouse","military","outbuilding",
    "pagoda","quonset_hut","roof","ruins","ship","tent","tower","triumphal_arch","windmill"
}

group_names = [
    "accommodation",
    "commercial",
    "religious",
    "amenity",
    "agriculture",
    "sports",
    "storage",
    "cars",
    "power_building",
    "other_building"
]
group_to_tags = {
    "accommodation": accommodation_tags,
    "commercial": commercial_tags,
    "religious": religious_tags,
    "amenity": amenity_tags,
    "agriculture": agriculture_tags,
    "sports": sports_tags,
    "storage": storage_tags,
    "cars": cars_tags,
    "power_building": power_building_tags,
    "other_building": other_building_tags
}

onehot_dim = len(group_names)  # 10

def get_group_index(b_type):
    """Return index (0..9) for building type b_type. Default to 'other_building' if not found."""
    if not isinstance(b_type, str):
        b_type = ""
    b_lower = b_type.lower()
    for idx, group in enumerate(group_names):
        if b_lower in group_to_tags[group]:
            return idx
    return group_names.index("other_building")

def group_to_onehot(g_idx):
    """Convert group index to one-hot array of length onehot_dim."""
    arr = np.zeros(onehot_dim, dtype=np.float64)  # matching float64 since rotation is float64
    arr[g_idx] = 1.0
    return arr

def rasterize_buildings(building_list, out_path, rotation=True, batch_size=10000):
    """
    Process the building_list in batches using np.memmap to avoid using too much RAM.
    Each building is rasterized into a 224x224 image, and for each building we store:
      - rotation parameters (cos, sin) if rotation is applied (else zeros)
      - building height
      - building type one-hot (10 dims)

    The output will be saved in two files:
      - building_raster.dat  (the 224x224 images)
      - building_rot_type.dat (for cos, sin, height, plus 10-dim one-hot => 13 floats total)

    Finally, we save metadata to npz files.
    """
    # We'll produce e.g. building_raster.npz and building_rot_type.npz at the end
    image_out_path = os.path.join(out_path, 'building_raster.npz')
    rot_type_out_path = os.path.join(out_path, 'building_rot_type.npz')

    N = len(building_list)
    print(f"Total buildings: {N}")

    # 224x224 images, shape => (N, 224, 224), dtype=uint8
    images_mem = np.memmap(
        os.path.join(out_path, 'building_raster.dat'),
        dtype=np.uint8, mode='w+',
        shape=(N, 224, 224)
    )
    # rotation + height + building type (10 dims) => total 3 + 10 = 13
    rot_type_dim = 3 + onehot_dim  # 3 = cos, sin, height
    rot_type_mem = np.memmap(
        os.path.join(out_path, 'building_rot_type.dat'),
        dtype=np.float64, mode='w+',
        shape=(N, rot_type_dim)
    )

    from tqdm import trange

    for start in trange(0, N, batch_size, desc="Processing batches"):
        end = min(start + batch_size, N)
        for i in range(start, end):
            building = building_list[i]
            polygon = building['shape']
            b_type = building.get('building', "")  # column building type
            # Convert building type to one-hot
            g_idx = get_group_index(b_type)
            oh_vec = group_to_onehot(g_idx)  # shape (10,)

            # If geometry is not a polygon but a point, buffer it
            if polygon.geom_type not in ['Polygon', 'MultiPolygon']:
                if polygon.geom_type == 'Point':
                    polygon = polygon.buffer(1.0)
                else:
                    continue

            cosp, sinp = 0.0, 0.0
            if rotation:
                rectangle = polygon.minimum_rotated_rectangle
                xc = polygon.centroid.x
                yc = polygon.centroid.y
                rec_x = []
                rec_y = []
                for point in rectangle.exterior.coords:
                    rec_x.append(point[0])
                    rec_y.append(point[1])
                top = np.argmax(rec_y)
                top_left = top - 1 if top > 0 else 3
                top_right = top + 1 if top < 3 else 0
                x0, y0 = rec_x[top], rec_y[top]
                x1, y1 = rec_x[top_left], rec_y[top_left]
                x2, y2 = rec_x[top_right], rec_y[top_right]
                d1 = np.linalg.norm([x0 - x1, y0 - y1])
                d2 = np.linalg.norm([x0 - x2, y0 - y2])
                if d1 > d2:
                    cosp = (x1 - x0) / d1
                    sinp = (y0 - y1) / d1
                else:
                    cosp = (x2 - x0) / d2
                    sinp = (y0 - y2) / d2
                matrix = (cosp, -sinp, 0.0,
                          sinp,  cosp, 0.0,
                          0.0,   0.0,  1.0,
                          xc - xc * cosp + yc * sinp, yc - xc * sinp - yc * cosp, 0.0)
                polygon = affinity.affine_transform(polygon, matrix)
            # store cos, sin in rot_type_mem
            rot_type_mem[i, 0] = cosp
            rot_type_mem[i, 1] = sinp

            height = building.get("height", np.nan)
            rot_type_mem[i, 2] = height  # store in 3rd column

            # store building-type one-hot in columns [3..12]
            rot_type_mem[i, 3:3+onehot_dim] = oh_vec  # oh_vec is float64 shape (10,)

            # Rasterize
            min_x, min_y, max_x, max_y = polygon.bounds
            length_x = max_x - min_x
            length_y = max_y - min_y
            if length_x > length_y:
                diff = length_x - length_y
                min_y -= diff / 2
                max_y += diff / 2
                length = length_x
            else:
                diff = length_y - length_x
                min_x -= diff / 2
                max_x += diff / 2
                length = length_y
            min_x -= length * 0.1
            min_y -= length * 0.1
            max_x += length * 0.1
            max_y += length * 0.1

            transform = rasterio.transform.from_bounds(min_x, min_y, max_x, max_y, 224, 224)
            image = rasterio.features.rasterize([polygon], out_shape=(224, 224), transform=transform)
            images_mem[i] = image

        # Flush batch
        images_mem.flush()
        rot_type_mem.flush()

    # Save compressed versions.
    np.savez_compressed(image_out_path, images_mem=images_mem)
    np.savez_compressed(rot_type_out_path, rot_type_mem=rot_type_mem)

    return images_mem, rot_type_mem

csv_file = "/content/drive/MyDrive/Research/USA data/Region embedding/Chicago/Building Footprint 2018 Chicago_new/Building list/combined_building_list_type.csv"
df = pd.read_csv(csv_file)

# Convert the 'geometry' column from WKT to Shapely geometries.
df['shape'] = df['geometry'].apply(wkt.loads)

# Convert the DataFrame to a list of dictionaries.
building_list = df.to_dict('records')

len(df)

# Define the output directory (update with your desired path).
out_path = "/content/drive/MyDrive/Research/USA data/Region embedding/Chicago/Building Footprint 2020 Chicago_new/Building list/"
if not os.path.exists(out_path):
    os.makedirs(out_path)

# Call the rasterize_buildings function.
images_mem, rot_height = rasterize_buildings(building_list, out_path=out_path, rotation=True, batch_size=10000)

print("Rasterization complete. Images shape:", images_mem.shape)

# Process only the first 10 images without loading the entire dataset into memory.
sample_images = images_mem[:10]
print("Sample images shape:", sample_images.shape)

import numpy as np
import matplotlib.pyplot as plt

# Define the output directory (update this to your actual path)
out_path = "/content/drive/MyDrive/Research/USA data/Region embedding/Chicago/Building Footprint 2020 Chicago_new/Building list/building_rot_type.npz"

data = np.load(out_path)
print("Keys in archive:", data.files)
rot_height = data['rot_type_mem']
print(rot_height.shape)

"""Resnet"""

import os
import random
import numpy as np
import timm
import torch
import torch.nn.functional as F
import torch.utils.data
from torch import nn
from torch.utils.data import Dataset
from tqdm import tqdm
import zipfile

class ImageDataset(Dataset):
    """
    This dataset loads building images from an npz file.
    The npz file is assumed to contain an array of shape (N, 224, 224)
    stored under the key 'arr_0'.
    This version extracts the internal .npy file from the npz archive and
    loads it using memory mapping.
    """
    def __init__(self, city):
        super(ImageDataset, self).__init__()
        print('Loading image data for {}...'.format(city))

        # Path to the compressed npz file.
        npz_file = '/content/drive/MyDrive/Research/USA data/Region embedding/Chicago/Building Footprint 2020 Chicago_new/Building list/building_raster.npz'

        # Define a temporary directory for extraction.
        extract_path = '/content/temp_building_images'
        if not os.path.exists(extract_path):
            os.makedirs(extract_path)

        # Open the npz file as a zip archive.
        with zipfile.ZipFile(npz_file, 'r') as zf:
            # List files in the archive; typically the images are stored under 'arr_0.npy'.
            npy_name = zf.namelist()[0]  # e.g., 'arr_0.npy'
            # Extract the .npy file to the temporary directory.
            zf.extract(npy_name, path=extract_path)

        npy_file = os.path.join(extract_path, npy_name)
        # Load the extracted .npy file with memory mapping.
        self.images = np.load(npy_file, mmap_mode="r")
        print('Image data loaded. Shape:', self.images.shape)

    def __getitem__(self, index):
        return self.images[index]

    def __len__(self):
        return self.images.shape[0]

    @staticmethod
    def collate_fn_augmentation(batch):
        result = []
        augmentations = [lambda x: x,
                         lambda x: np.flip(x, axis=0),
                         lambda x: np.flip(x, axis=1),
                         lambda x: np.rot90(x, k=1, axes=(0, 1)),
                         lambda x: np.rot90(x, k=2, axes=(0, 1)),
                         lambda x: np.rot90(x, k=3, axes=(0, 1))]
        for pic in batch:
            choice1 = random.choice(augmentations)
            choice2 = random.choice(augmentations)
            result.append(choice1(pic)[np.newaxis, :, :])
            result.append(choice2(pic)[np.newaxis, :, :])
        return np.concatenate(result, axis=0)

    @staticmethod
    def collate_fn_embed(batch):
        # For inference/embedding, simply stack images along the batch axis.
        return np.vstack([pic[np.newaxis, :, :] for pic in batch])

class ResNet(torch.nn.Module):
    def __init__(self, **kwargs):
        super(ResNet, self).__init__()
        net = timm.create_model('resnet18', pretrained=True, **kwargs)
        # Remove the final fully-connected layer.
        self.net = nn.Sequential(*(list(net.children())[:-1]))
        self.projector = nn.Sequential(
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Linear(1024, 1024),
            nn.ReLU(),
            nn.Linear(1024, 64))  # Final embedding dimension is 64.


    def forward(self, x):
        return self.projector(self.get_feature(x))

    def get_feature(self, x):
        x = x.unsqueeze(1).repeat(1, 3, 1, 1)  # Convert grayscale to 3-channel
        return self.net(x)

class SimCLRTrainer(object):
    def __init__(self, city):
        self.data = ImageDataset(city)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = ResNet().to(self.device)
        self.criterion = self.infonce_loss
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0001)
        self.train_loader = torch.utils.data.DataLoader(
            self.data, batch_size=128, shuffle=True, collate_fn=ImageDataset.collate_fn_augmentation)
        self.test_loader = torch.utils.data.DataLoader(
            self.data, batch_size=128, shuffle=False, collate_fn=ImageDataset.collate_fn_embed)

    def train(self, epochs):
        self.model.train()
        for epoch in range(epochs):
            losses = []
            with tqdm(self.train_loader, total=len(self.train_loader)) as t:
                for x in t:
                    x = torch.from_numpy(x).float().to(self.device)
                    self.optimizer.zero_grad()
                    y_pred = self.model(x)
                    loss = self.criterion(y_pred)
                    loss.backward()
                    self.optimizer.step()
                    t.set_description(f'Epoch {epoch} loss: {loss.item()}')
                    losses.append(loss.item())
            print(f'Epoch {epoch} loss: {np.mean(losses)}')

    def embed(self):
        self.model.eval()
        embeddings = []
        with torch.no_grad():
            for x in self.test_loader:
                x = torch.from_numpy(x).float().to(self.device)
                # embeddings.append(self.model.get_feature(x).cpu().numpy())
                embeddings.append(self.model(x).cpu().numpy())   #to get 64 dimension
        return np.concatenate(embeddings, axis=0)

    def infonce_loss(self, y_pred, lamda=0.05):
        idxs = torch.arange(0, y_pred.shape[0], device=self.device)
        y_true = idxs + 1 - idxs % 2 * 2
        similarities = F.cosine_similarity(y_pred.unsqueeze(1), y_pred.unsqueeze(0), dim=2)
        similarities = similarities - torch.eye(y_pred.shape[0], device=self.device) * 1e12
        similarities = similarities / lamda
        loss = F.cross_entropy(similarities, y_true)
        return torch.mean(loss)

def train_unsupervised(city):
    trainer = SimCLRTrainer(city)
    # trainer.train(3)
    embeddings = trainer.embed()
    # Save the computed embeddings.
    np.save(f'/content/drive/MyDrive/Research/USA data/Region embedding/Chicago/Building Footprint 2020 Chicago_new/Building list/processed/building_features.npy', embeddings)

if __name__ == '__main__':
    city = 'Chicago'
    train_unsupervised(city)

import numpy as np

embeddings = np.load('/content/drive/MyDrive/Research/USA data/Region embedding/Chicago/Building Footprint 2020 Chicago_new/Building list/processed/building_features.npy')
print("Embeddings shape:", embeddings.shape)

"""# New Section"""

import os
import numpy as np
import pandas as pd


accommodation_tags = {
    "yes","apartments","barracks","bungalow","cabin","detached","annexe","dormitory","farm","ger",
    "hotel","house","houseboat","residential","semidetached_house","static_caravan","stilt_house",
    "terrace","tree_house","trullo"
}
commercial_tags = {"commercial","industrial","kiosk","office","retail","supermarket","warehouse"}
religious_tags = {"religious","cathedral","chapel","church","kingdom_hall","monastery","mosque",
                  "presbytery","shrine","synagogue","temple"}
amenity_tags = {"bakehouse","bridge","civic","college","fire_station","government","gatehouse","hospital",
                "kindergarten","museum","public","school","toilets","train_station","transportation","university"}
agriculture_tags = {"barn","conservatory","cowshed","farm_auxiliary","greenhouse","slurry_tank","stable","sty","livestock"}
sports_tags = {"grandstand","pavilion","riding_hall","sports_hall","sports_centre","stadium"}
storage_tags = {"allotment_house","boathouse","hanger","hut","shed"}
cars_tags = {"carport","garage","garages","parking"}
power_building_tags = {"digester","service","tech_cab","transformer_tower","water_tower","silo","storage_tank"}
other_building_tags = {
    "beach_hut","bunker","castle","construction","container","guardhouse","military","outbuilding",
    "pagoda","quonset_hut","roof","ruins","ship","tent","tower","triumphal_arch","windmill"
}

group_names = [
    "accommodation",
    "commercial",
    "religious",
    "amenity",
    "agriculture",
    "sports",
    "storage",
    "cars",
    "power_building",
    "other_building"
]
group_to_tags = {
    "accommodation": accommodation_tags,
    "commercial": commercial_tags,
    "religious": religious_tags,
    "amenity": amenity_tags,
    "agriculture": agriculture_tags,
    "sports": sports_tags,
    "storage": storage_tags,
    "cars": cars_tags,
    "power_building": power_building_tags,
    "other_building": other_building_tags
}

onehot_dim = len(group_names)  # 10

# Precompute a {tag -> group_idx} dictionary for O(1) lookup
tag_to_group_idx = {}
for group_idx, group in enumerate(group_names):
    for tag in group_to_tags[group]:
        tag_to_group_idx[tag] = group_idx

def get_group_index(b_type):
    """Return index (0..9) for building type b_type. Defaults to 'other_building' if not found."""
    if not isinstance(b_type, str):
        return group_names.index("other_building")
    return tag_to_group_idx.get(b_type.lower(), group_names.index("other_building"))

def batch_to_onehot(building_types):
    """Convert a list of building types to a (N, 10) one-hot array."""
    group_indices = [get_group_index(b_type) for b_type in building_types]
    onehot = np.zeros((len(building_types), onehot_dim), dtype=np.float64)
    onehot[np.arange(len(building_types)), group_indices] = 1.0
    return onehot

def group_to_onehot(g_idx):
    """Convert group index to one-hot array. Validate input."""
    assert 0 <= g_idx < onehot_dim, f"Invalid group index: {g_idx}"
    arr = np.zeros(onehot_dim, dtype=np.float64)
    arr[g_idx] = 1.0
    return arr

df.columns

# Test cases
print(get_group_index("house"))          # 0 (accommodation)
print(get_group_index("garage"))        # 7 (cars)
print(get_group_index("unknown_tag"))   # 9 (other_building)
print(get_group_index(None))            # 9 (other_building)

# One-hot encoding
print(group_to_onehot(0))  # [1. 0. 0. ... 0.] (accommodation)
print(group_to_onehot(7))  # [0. 0. ... 1. 0.] (cars)

# Batch processing
building_types = ["house", "garage", "school", None]
onehot_array = batch_to_onehot(building_types)
print(onehot_array)